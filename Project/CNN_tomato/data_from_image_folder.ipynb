{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import visdom\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "no_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.ReLU(inplace=True),\n",
    "    \n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_bn(  3,  32, 2), \n",
    "            conv_dw( 32,  64, 1),\n",
    "            conv_dw( 64, 128, 2),\n",
    "            conv_dw(128, 128, 1),\n",
    "            conv_dw(128, 256, 2),\n",
    "            conv_dw(256, 256, 1),\n",
    "            conv_dw(256, 512, 2),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 1024, 2),\n",
    "            conv_dw(1024, 1024, 1),\n",
    "            nn.AvgPool2d(3),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]             864\n",
      "       BatchNorm2d-2           [-1, 32, 64, 64]              64\n",
      "              ReLU-3           [-1, 32, 64, 64]               0\n",
      "            Conv2d-4           [-1, 32, 64, 64]             288\n",
      "       BatchNorm2d-5           [-1, 32, 64, 64]              64\n",
      "              ReLU-6           [-1, 32, 64, 64]               0\n",
      "            Conv2d-7           [-1, 64, 64, 64]           2,048\n",
      "       BatchNorm2d-8           [-1, 64, 64, 64]             128\n",
      "              ReLU-9           [-1, 64, 64, 64]               0\n",
      "           Conv2d-10           [-1, 64, 32, 32]             576\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "             ReLU-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-14          [-1, 128, 32, 32]             256\n",
      "             ReLU-15          [-1, 128, 32, 32]               0\n",
      "           Conv2d-16          [-1, 128, 32, 32]           1,152\n",
      "      BatchNorm2d-17          [-1, 128, 32, 32]             256\n",
      "             ReLU-18          [-1, 128, 32, 32]               0\n",
      "           Conv2d-19          [-1, 128, 32, 32]          16,384\n",
      "      BatchNorm2d-20          [-1, 128, 32, 32]             256\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22          [-1, 128, 16, 16]           1,152\n",
      "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
      "             ReLU-24          [-1, 128, 16, 16]               0\n",
      "           Conv2d-25          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-26          [-1, 256, 16, 16]             512\n",
      "             ReLU-27          [-1, 256, 16, 16]               0\n",
      "           Conv2d-28          [-1, 256, 16, 16]           2,304\n",
      "      BatchNorm2d-29          [-1, 256, 16, 16]             512\n",
      "             ReLU-30          [-1, 256, 16, 16]               0\n",
      "           Conv2d-31          [-1, 256, 16, 16]          65,536\n",
      "      BatchNorm2d-32          [-1, 256, 16, 16]             512\n",
      "             ReLU-33          [-1, 256, 16, 16]               0\n",
      "           Conv2d-34            [-1, 256, 8, 8]           2,304\n",
      "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
      "             ReLU-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-38            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-39            [-1, 512, 8, 8]               0\n",
      "           Conv2d-40            [-1, 512, 8, 8]           4,608\n",
      "      BatchNorm2d-41            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-42            [-1, 512, 8, 8]               0\n",
      "           Conv2d-43            [-1, 512, 8, 8]         262,144\n",
      "      BatchNorm2d-44            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-45            [-1, 512, 8, 8]               0\n",
      "           Conv2d-46            [-1, 512, 8, 8]           4,608\n",
      "      BatchNorm2d-47            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-48            [-1, 512, 8, 8]               0\n",
      "           Conv2d-49            [-1, 512, 8, 8]         262,144\n",
      "      BatchNorm2d-50            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-51            [-1, 512, 8, 8]               0\n",
      "           Conv2d-52            [-1, 512, 8, 8]           4,608\n",
      "      BatchNorm2d-53            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-54            [-1, 512, 8, 8]               0\n",
      "           Conv2d-55            [-1, 512, 8, 8]         262,144\n",
      "      BatchNorm2d-56            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-57            [-1, 512, 8, 8]               0\n",
      "           Conv2d-58            [-1, 512, 8, 8]           4,608\n",
      "      BatchNorm2d-59            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-60            [-1, 512, 8, 8]               0\n",
      "           Conv2d-61            [-1, 512, 8, 8]         262,144\n",
      "      BatchNorm2d-62            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-63            [-1, 512, 8, 8]               0\n",
      "           Conv2d-64            [-1, 512, 8, 8]           4,608\n",
      "      BatchNorm2d-65            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-66            [-1, 512, 8, 8]               0\n",
      "           Conv2d-67            [-1, 512, 8, 8]         262,144\n",
      "      BatchNorm2d-68            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-69            [-1, 512, 8, 8]               0\n",
      "           Conv2d-70            [-1, 512, 4, 4]           4,608\n",
      "      BatchNorm2d-71            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-72            [-1, 512, 4, 4]               0\n",
      "           Conv2d-73           [-1, 1024, 4, 4]         524,288\n",
      "      BatchNorm2d-74           [-1, 1024, 4, 4]           2,048\n",
      "             ReLU-75           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-76           [-1, 1024, 4, 4]           9,216\n",
      "      BatchNorm2d-77           [-1, 1024, 4, 4]           2,048\n",
      "             ReLU-78           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-79           [-1, 1024, 4, 4]       1,048,576\n",
      "      BatchNorm2d-80           [-1, 1024, 4, 4]           2,048\n",
      "             ReLU-81           [-1, 1024, 4, 4]               0\n",
      "        AvgPool2d-82           [-1, 1024, 1, 1]               0\n",
      "           Linear-83                    [-1, 9]           9,225\n",
      "================================================================\n",
      "Total params: 3,216,201\n",
      "Trainable params: 3,216,201\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 37.70\n",
      "Params size (MB): 12.27\n",
      "Estimated Total Size (MB): 50.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'dataset/tomato/'\n",
    "# test_dir = 'dataset/mnist/testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=train_dir,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    ]))\n",
    "# test_dataset = datasets.ImageFolder(root=test_dir,\n",
    "#                                     transform=transforms.Compose([\n",
    "#                                         transforms.ToTensor(),\n",
    "#                                         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                                     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                           batch_size = batch_size,\n",
    "#                                           shuffle = True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "#                                           batch_size = batch_size,\n",
    "#                                           shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size = len(train_dataset.imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17063, 3, 128, 128])\n",
      "torch.Size([17063])\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_loader:\n",
    "    image = image\n",
    "    label = label\n",
    "    print(image.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = image.view(-1,128*128*3)\n",
    "target = label.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_x = data.numpy()\n",
    "dataset_y = target.numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('./dataset/image.npy', dataset_x)\n",
    "np.save('./dataset/label.npy', dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataset_x = np.load('./dataset/image.npy')\n",
    "dataset_y = np.load('./dataset/label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(dataset_x, dataset_y, test_size=0.1, random_state=1,stratify=dataset_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15356, 49152)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15356, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 6, 6, 6, 6, 2, 6, 6, 6, 7, 6, 6, 6, 2, 7, 6, 6, 7, 6, 7, 7,\n",
       "       7, 7, 7, 6, 6, 6, 6, 4, 6, 6, 6, 7, 2, 2, 7, 6, 6, 6, 6, 2, 6, 6,\n",
       "       7, 6, 6, 7, 7, 2, 2, 7, 6, 6, 2, 6, 6, 2, 2, 7, 6, 2, 6, 1, 7, 2,\n",
       "       2, 4, 6, 6, 1, 6, 6, 6, 6, 4, 6, 6, 7, 6, 6, 6, 7, 7, 6, 6, 6, 6,\n",
       "       2, 6, 7, 7, 1, 7, 6, 6, 6, 7, 6, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:100,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train).view([-1,3,128,128])\n",
    "y_train = torch.LongTensor(y_train).view([-1])\n",
    "X_val = torch.FloatTensor(X_val).view([-1,3,128,128])\n",
    "y_val = torch.LongTensor(y_val).view([-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 23,  77, 206,   9,  42,   5, 996, 335,  14]),\n",
       " tensor([ 206,  689, 1855,   81,  384,   44, 8959, 3013,  125]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.bincount(), y_train.bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15356, 3, 128, 128]), torch.Size([15356]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "validDataset = torch.utils.data.TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_loader = torch.utils.data.DataLoader(trainDataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = False)\n",
    "Valid_loader = torch.utils.data.DataLoader(validDataset,\n",
    "                                          batch_size = len(X_val),\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-15 02:15:02.550798\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.002\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(),lr=lr,momentum=momentum)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(Train_loader))\n",
    "writer = SummaryWriter(comment='Model')\n",
    "writer.add_graph(Net(), input_to_model=images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 / batch:50 / loss 1.26\n",
      "Epoch:1 / batch:100 / loss 1.13\n",
      "Epoch 1 / time 0:00:38.467473 / loss 1.16 / val_loss 1.21 / acc 57.09 / val_acc 58.35\n",
      "Epoch:2 / batch:50 / loss 1.17\n",
      "Epoch:2 / batch:100 / loss 1.08\n",
      "Epoch 2 / time 0:00:37.614701 / loss 1.16 / val_loss 1.17 / acc 57.73 / val_acc 58.88\n",
      "Epoch:3 / batch:50 / loss 1.10\n",
      "Epoch:3 / batch:100 / loss 1.00\n",
      "Epoch 3 / time 0:00:37.644803 / loss 1.09 / val_loss 1.06 / acc 58.50 / val_acc 64.32\n",
      "Epoch:4 / batch:50 / loss 0.88\n",
      "Epoch:4 / batch:100 / loss 0.87\n",
      "Epoch 4 / time 0:00:37.618618 / loss 0.93 / val_loss 0.91 / acc 60.92 / val_acc 71.94\n",
      "Epoch:5 / batch:50 / loss 0.80\n",
      "Epoch:5 / batch:100 / loss 0.79\n",
      "Epoch 5 / time 0:00:37.619600 / loss 0.85 / val_loss 0.86 / acc 63.34 / val_acc 74.40\n",
      "Epoch:6 / batch:50 / loss 0.68\n",
      "Epoch:6 / batch:100 / loss 0.78\n",
      "Epoch 6 / time 0:00:37.666696 / loss 0.79 / val_loss 0.81 / acc 65.23 / val_acc 76.80\n",
      "Epoch:7 / batch:50 / loss 0.67\n",
      "Epoch:7 / batch:100 / loss 0.79\n",
      "Epoch:9 / batch:50 / loss 0.57\n",
      "Epoch:9 / batch:100 / loss 0.74\n",
      "Epoch 9 / time 0:00:37.600362 / loss 0.69 / val_loss 0.73 / acc 69.20 / val_acc 79.44\n",
      "Epoch:10 / batch:50 / loss 0.54\n",
      "Epoch:10 / batch:100 / loss 0.71\n",
      "Epoch 10 / time 0:00:37.572281 / loss 0.72 / val_loss 0.71 / acc 70.09 / val_acc 80.32\n",
      "Epoch:11 / batch:50 / loss 0.53\n",
      "Epoch:11 / batch:100 / loss 0.69\n",
      "Epoch 11 / time 0:00:37.631530 / loss 0.69 / val_loss 0.72 / acc 70.90 / val_acc 80.61\n",
      "Epoch:12 / batch:50 / loss 0.50\n",
      "Epoch:12 / batch:100 / loss 0.68\n",
      "Epoch 12 / time 0:00:37.594921 / loss 0.66 / val_loss 0.70 / acc 71.60 / val_acc 81.08\n",
      "Epoch:13 / batch:50 / loss 0.52\n",
      "Epoch:13 / batch:100 / loss 0.70\n",
      "Epoch 13 / time 0:00:37.631082 / loss 0.63 / val_loss 0.68 / acc 72.24 / val_acc 81.02\n",
      "Epoch:14 / batch:50 / loss 0.45\n",
      "Epoch:14 / batch:100 / loss 0.65\n",
      "Epoch 14 / time 0:00:37.586969 / loss 0.62 / val_loss 0.68 / acc 72.78 / val_acc 80.96\n",
      "Epoch:15 / batch:50 / loss 0.41\n",
      "Epoch:15 / batch:100 / loss 0.64\n",
      "Epoch 15 / time 0:00:37.606650 / loss 0.61 / val_loss 0.67 / acc 73.29 / val_acc 81.55\n",
      "Epoch:16 / batch:50 / loss 0.45\n",
      "Epoch:16 / batch:100 / loss 0.62\n",
      "Epoch 16 / time 0:00:37.559381 / loss 0.61 / val_loss 0.66 / acc 73.76 / val_acc 81.02\n",
      "Epoch:17 / batch:50 / loss 0.44\n",
      "Epoch:17 / batch:100 / loss 0.59\n",
      "Epoch 17 / time 0:00:37.645900 / loss 0.57 / val_loss 0.65 / acc 74.20 / val_acc 81.37\n",
      "Epoch:18 / batch:50 / loss 0.43\n",
      "Epoch:18 / batch:100 / loss 0.59\n",
      "Epoch 18 / time 0:00:37.626640 / loss 0.57 / val_loss 0.65 / acc 74.60 / val_acc 81.25\n",
      "Epoch:19 / batch:50 / loss 0.38\n",
      "Epoch:19 / batch:100 / loss 0.62\n",
      "Epoch 19 / time 0:00:37.625396 / loss 0.57 / val_loss 0.64 / acc 74.97 / val_acc 81.55\n",
      "Epoch:20 / batch:50 / loss 0.40\n",
      "Epoch:20 / batch:100 / loss 0.59\n",
      "Epoch 20 / time 0:00:37.632490 / loss 0.54 / val_loss 0.64 / acc 75.33 / val_acc 81.84\n",
      "Epoch:21 / batch:50 / loss 0.38\n",
      "Epoch:21 / batch:100 / loss 0.57\n",
      "Epoch 21 / time 0:00:37.637762 / loss 0.54 / val_loss 0.63 / acc 75.69 / val_acc 82.37\n",
      "Epoch:22 / batch:50 / loss 0.36\n",
      "Epoch:22 / batch:100 / loss 0.60\n",
      "Epoch 22 / time 0:00:37.636651 / loss 0.53 / val_loss 0.63 / acc 76.02 / val_acc 81.96\n",
      "Epoch:23 / batch:50 / loss 0.34\n",
      "Epoch:23 / batch:100 / loss 0.52\n",
      "Epoch 23 / time 0:00:37.622823 / loss 0.47 / val_loss 0.63 / acc 76.31 / val_acc 81.90\n",
      "Epoch:24 / batch:50 / loss 0.34\n",
      "Epoch:24 / batch:100 / loss 0.54\n",
      "Epoch 24 / time 0:00:37.708185 / loss 0.51 / val_loss 0.62 / acc 76.60 / val_acc 81.78\n",
      "Epoch:25 / batch:50 / loss 0.31\n",
      "Epoch:25 / batch:100 / loss 0.56\n",
      "Epoch 25 / time 0:00:37.564541 / loss 0.48 / val_loss 0.62 / acc 76.87 / val_acc 82.25\n",
      "Epoch:26 / batch:50 / loss 0.32\n",
      "Epoch:26 / batch:100 / loss 0.50\n",
      "Epoch 26 / time 0:00:37.624136 / loss 0.48 / val_loss 0.62 / acc 77.15 / val_acc 81.66\n",
      "Epoch:27 / batch:50 / loss 0.34\n",
      "Epoch:27 / batch:100 / loss 0.50\n",
      "Epoch 27 / time 0:00:37.631885 / loss 0.43 / val_loss 0.63 / acc 77.42 / val_acc 82.07\n",
      "Epoch:28 / batch:50 / loss 0.31\n",
      "Epoch:28 / batch:100 / loss 0.46\n",
      "Epoch 28 / time 0:00:37.676580 / loss 0.44 / val_loss 0.63 / acc 77.70 / val_acc 81.61\n",
      "Epoch    27: reducing learning rate of group 0 to 1.8000e-03.\n",
      "Epoch:29 / batch:50 / loss 0.33\n",
      "Epoch:29 / batch:100 / loss 0.45\n",
      "Epoch 29 / time 0:00:37.609408 / loss 0.41 / val_loss 0.63 / acc 77.98 / val_acc 82.37\n",
      "Epoch:30 / batch:50 / loss 0.26\n",
      "Epoch:30 / batch:100 / loss 0.36\n",
      "Epoch 30 / time 0:00:37.566823 / loss 0.37 / val_loss 0.63 / acc 78.27 / val_acc 82.31\n",
      "Epoch    29: reducing learning rate of group 0 to 1.6200e-03.\n",
      "Epoch:31 / batch:50 / loss 0.23\n",
      "Epoch:31 / batch:100 / loss 0.32\n",
      "Epoch 31 / time 0:00:37.517858 / loss 0.32 / val_loss 0.63 / acc 78.58 / val_acc 82.19\n",
      "Epoch:32 / batch:50 / loss 0.21\n",
      "Epoch:32 / batch:100 / loss 0.27\n",
      "Epoch 32 / time 0:00:37.620811 / loss 0.30 / val_loss 0.66 / acc 78.93 / val_acc 80.73\n",
      "Epoch    31: reducing learning rate of group 0 to 1.4580e-03.\n",
      "Epoch:33 / batch:50 / loss 0.18\n",
      "Epoch:33 / batch:100 / loss 0.22\n",
      "Epoch 33 / time 0:00:37.555357 / loss 0.23 / val_loss 0.68 / acc 79.31 / val_acc 81.20\n",
      "Epoch:34 / batch:50 / loss 0.14\n",
      "Epoch:34 / batch:100 / loss 0.18\n",
      "Epoch 34 / time 0:00:37.628178 / loss 0.21 / val_loss 0.71 / acc 79.71 / val_acc 81.43\n",
      "Epoch    33: reducing learning rate of group 0 to 1.3122e-03.\n",
      "Epoch:35 / batch:50 / loss 0.10\n",
      "Epoch:35 / batch:100 / loss 0.12\n",
      "Epoch 35 / time 0:00:37.717280 / loss 0.17 / val_loss 0.73 / acc 80.14 / val_acc 81.49\n",
      "Epoch:36 / batch:50 / loss 0.09\n",
      "Epoch:36 / batch:100 / loss 0.08\n",
      "Epoch 36 / time 0:00:37.642199 / loss 0.14 / val_loss 0.76 / acc 80.58 / val_acc 81.49\n",
      "Epoch    35: reducing learning rate of group 0 to 1.1810e-03.\n",
      "Epoch:37 / batch:50 / loss 0.07\n",
      "Epoch:37 / batch:100 / loss 0.05\n",
      "Epoch 37 / time 0:00:37.599841 / loss 0.13 / val_loss 0.77 / acc 81.02 / val_acc 81.61\n",
      "Epoch:38 / batch:50 / loss 0.06\n",
      "Epoch:38 / batch:100 / loss 0.04\n",
      "Epoch 38 / time 0:00:37.560592 / loss 0.11 / val_loss 0.79 / acc 81.45 / val_acc 81.14\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0629e-03.\n",
      "Epoch:39 / batch:50 / loss 0.05\n",
      "Epoch:39 / batch:100 / loss 0.03\n",
      "Epoch 39 / time 0:00:37.655425 / loss 0.10 / val_loss 0.81 / acc 81.87 / val_acc 81.55\n",
      "Epoch:40 / batch:50 / loss 0.04\n",
      "Epoch:40 / batch:100 / loss 0.03\n",
      "Epoch 40 / time 0:00:37.631116 / loss 0.09 / val_loss 0.82 / acc 82.28 / val_acc 81.49\n",
      "Epoch    39: reducing learning rate of group 0 to 9.5659e-04.\n",
      "Epoch:41 / batch:50 / loss 0.04\n",
      "Epoch:41 / batch:100 / loss 0.03\n",
      "Epoch 41 / time 0:00:37.609352 / loss 0.08 / val_loss 0.84 / acc 82.67 / val_acc 81.25\n",
      "Epoch:42 / batch:50 / loss 0.03\n",
      "Epoch:42 / batch:100 / loss 0.03\n",
      "Epoch 42 / time 0:00:37.597817 / loss 0.07 / val_loss 0.85 / acc 83.05 / val_acc 81.31\n",
      "Epoch    41: reducing learning rate of group 0 to 8.6093e-04.\n",
      "Epoch:43 / batch:50 / loss 0.04\n",
      "Epoch:43 / batch:100 / loss 0.03\n",
      "Epoch 43 / time 0:00:37.581677 / loss 0.07 / val_loss 0.85 / acc 83.40 / val_acc 81.31\n",
      "Epoch:44 / batch:50 / loss 0.03\n",
      "Epoch:44 / batch:100 / loss 0.02\n",
      "Epoch 44 / time 0:00:37.662503 / loss 0.06 / val_loss 0.86 / acc 83.75 / val_acc 81.49\n",
      "Epoch    43: reducing learning rate of group 0 to 7.7484e-04.\n",
      "Epoch:45 / batch:50 / loss 0.03\n",
      "Epoch:45 / batch:100 / loss 0.02\n",
      "Epoch 45 / time 0:00:37.646865 / loss 0.06 / val_loss 0.87 / acc 84.08 / val_acc 81.43\n",
      "Epoch:46 / batch:50 / loss 0.03\n",
      "Epoch:46 / batch:100 / loss 0.02\n",
      "Epoch 46 / time 0:00:37.625212 / loss 0.06 / val_loss 0.88 / acc 84.40 / val_acc 81.37\n",
      "Epoch    45: reducing learning rate of group 0 to 6.9736e-04.\n",
      "Epoch:47 / batch:50 / loss 0.02\n",
      "Epoch:47 / batch:100 / loss 0.02\n",
      "Epoch 47 / time 0:00:37.590677 / loss 0.06 / val_loss 0.89 / acc 84.71 / val_acc 81.02\n",
      "Epoch:48 / batch:50 / loss 0.02\n",
      "Epoch:48 / batch:100 / loss 0.02\n",
      "Epoch 48 / time 0:00:37.603046 / loss 0.05 / val_loss 0.89 / acc 85.01 / val_acc 81.14\n",
      "Epoch    47: reducing learning rate of group 0 to 6.2762e-04.\n",
      "Epoch:49 / batch:50 / loss 0.02\n",
      "Epoch:49 / batch:100 / loss 0.02\n",
      "Epoch 49 / time 0:00:37.552731 / loss 0.05 / val_loss 0.90 / acc 85.30 / val_acc 81.08\n",
      "Epoch:50 / batch:50 / loss 0.02\n",
      "Epoch:50 / batch:100 / loss 0.01\n",
      "Epoch 50 / time 0:00:37.601515 / loss 0.05 / val_loss 0.90 / acc 85.58 / val_acc 81.14\n",
      "Epoch    49: reducing learning rate of group 0 to 5.6486e-04.\n",
      "Epoch:51 / batch:50 / loss 0.02\n",
      "Epoch:51 / batch:100 / loss 0.01\n",
      "Epoch 51 / time 0:00:37.605517 / loss 0.05 / val_loss 0.91 / acc 85.85 / val_acc 81.20\n",
      "Epoch:52 / batch:50 / loss 0.02\n",
      "Epoch:52 / batch:100 / loss 0.01\n",
      "Epoch 52 / time 0:00:37.649062 / loss 0.05 / val_loss 0.91 / acc 86.12 / val_acc 81.08\n",
      "Epoch    51: reducing learning rate of group 0 to 5.0837e-04.\n",
      "Epoch:53 / batch:50 / loss 0.02\n",
      "Epoch:53 / batch:100 / loss 0.01\n",
      "Epoch 53 / time 0:00:37.623123 / loss 0.05 / val_loss 0.92 / acc 86.37 / val_acc 80.96\n",
      "Epoch:54 / batch:50 / loss 0.02\n",
      "Epoch:54 / batch:100 / loss 0.01\n",
      "Epoch 54 / time 0:00:37.753959 / loss 0.04 / val_loss 0.92 / acc 86.61 / val_acc 80.96\n",
      "Epoch    53: reducing learning rate of group 0 to 4.5754e-04.\n",
      "Epoch:55 / batch:50 / loss 0.02\n",
      "Epoch:55 / batch:100 / loss 0.01\n",
      "Epoch 55 / time 0:00:37.726058 / loss 0.04 / val_loss 0.93 / acc 86.85 / val_acc 81.37\n",
      "Epoch:56 / batch:50 / loss 0.02\n",
      "Epoch:56 / batch:100 / loss 0.01\n",
      "Epoch 56 / time 0:00:37.624966 / loss 0.04 / val_loss 0.93 / acc 87.07 / val_acc 81.25\n",
      "Epoch    55: reducing learning rate of group 0 to 4.1178e-04.\n",
      "Epoch:57 / batch:50 / loss 0.02\n",
      "Epoch:57 / batch:100 / loss 0.01\n",
      "Epoch 57 / time 0:00:37.820780 / loss 0.04 / val_loss 0.93 / acc 87.29 / val_acc 81.31\n",
      "Epoch:58 / batch:50 / loss 0.02\n",
      "Epoch:58 / batch:100 / loss 0.01\n",
      "Epoch 58 / time 0:00:37.549594 / loss 0.04 / val_loss 0.94 / acc 87.51 / val_acc 81.43\n",
      "Epoch    57: reducing learning rate of group 0 to 3.7060e-04.\n",
      "Epoch:59 / batch:50 / loss 0.02\n",
      "Epoch:59 / batch:100 / loss 0.02\n",
      "Epoch 59 / time 0:00:37.667060 / loss 0.04 / val_loss 0.94 / acc 87.71 / val_acc 81.31\n",
      "Epoch:60 / batch:50 / loss 0.02\n",
      "Epoch:60 / batch:100 / loss 0.02\n",
      "Epoch 60 / time 0:00:37.824325 / loss 0.04 / val_loss 0.95 / acc 87.91 / val_acc 81.37\n",
      "Epoch    59: reducing learning rate of group 0 to 3.3354e-04.\n",
      "Epoch:61 / batch:50 / loss 0.02\n",
      "Epoch:61 / batch:100 / loss 0.02\n",
      "Epoch 61 / time 0:00:37.713848 / loss 0.04 / val_loss 0.95 / acc 88.10 / val_acc 81.08\n",
      "Epoch:62 / batch:50 / loss 0.02\n",
      "Epoch:62 / batch:100 / loss 0.02\n",
      "Epoch 62 / time 0:00:37.635779 / loss 0.04 / val_loss 0.95 / acc 88.29 / val_acc 81.02\n",
      "Epoch    61: reducing learning rate of group 0 to 3.0019e-04.\n",
      "Epoch:63 / batch:50 / loss 0.02\n",
      "Epoch:63 / batch:100 / loss 0.02\n",
      "Epoch 63 / time 0:00:37.615474 / loss 0.04 / val_loss 0.96 / acc 88.47 / val_acc 81.02\n",
      "Epoch:64 / batch:50 / loss 0.02\n",
      "Epoch:64 / batch:100 / loss 0.02\n",
      "Epoch 64 / time 0:00:37.630253 / loss 0.04 / val_loss 0.96 / acc 88.64 / val_acc 80.73\n",
      "Epoch    63: reducing learning rate of group 0 to 2.7017e-04.\n",
      "Epoch:65 / batch:50 / loss 0.02\n",
      "Epoch:65 / batch:100 / loss 0.02\n",
      "Epoch 65 / time 0:00:37.565879 / loss 0.04 / val_loss 0.96 / acc 88.81 / val_acc 80.67\n",
      "Epoch:66 / batch:50 / loss 0.02\n",
      "Epoch:66 / batch:100 / loss 0.02\n",
      "Epoch 66 / time 0:00:37.628879 / loss 0.04 / val_loss 0.97 / acc 88.98 / val_acc 80.67\n",
      "Epoch    65: reducing learning rate of group 0 to 2.4315e-04.\n",
      "Epoch:67 / batch:50 / loss 0.02\n",
      "Epoch:67 / batch:100 / loss 0.02\n",
      "Epoch 67 / time 0:00:37.839125 / loss 0.05 / val_loss 0.97 / acc 89.14 / val_acc 80.90\n",
      "Epoch:68 / batch:50 / loss 0.02\n",
      "Epoch:68 / batch:100 / loss 0.02\n",
      "Epoch 68 / time 0:00:37.675317 / loss 0.05 / val_loss 0.97 / acc 89.29 / val_acc 80.96\n",
      "Epoch    67: reducing learning rate of group 0 to 2.1884e-04.\n",
      "Epoch:69 / batch:50 / loss 0.02\n",
      "Epoch:69 / batch:100 / loss 0.02\n",
      "Epoch 69 / time 0:00:37.555976 / loss 0.05 / val_loss 0.97 / acc 89.44 / val_acc 80.90\n",
      "Epoch:70 / batch:50 / loss 0.02\n",
      "Epoch:70 / batch:100 / loss 0.02\n",
      "Epoch 70 / time 0:00:37.614990 / loss 0.05 / val_loss 0.97 / acc 89.59 / val_acc 80.84\n",
      "Epoch    69: reducing learning rate of group 0 to 1.9695e-04.\n",
      "Epoch:71 / batch:50 / loss 0.02\n",
      "Epoch:71 / batch:100 / loss 0.02\n",
      "Epoch 71 / time 0:00:37.750408 / loss 0.05 / val_loss 0.97 / acc 89.73 / val_acc 80.96\n",
      "Epoch:72 / batch:50 / loss 0.02\n",
      "Epoch:72 / batch:100 / loss 0.02\n",
      "Epoch 72 / time 0:00:38.042108 / loss 0.05 / val_loss 0.97 / acc 89.86 / val_acc 81.14\n",
      "Epoch    71: reducing learning rate of group 0 to 1.7726e-04.\n",
      "Epoch:73 / batch:50 / loss 0.02\n",
      "Epoch:73 / batch:100 / loss 0.02\n",
      "Epoch 73 / time 0:00:37.557035 / loss 0.05 / val_loss 0.97 / acc 90.00 / val_acc 81.08\n",
      "Epoch:74 / batch:50 / loss 0.02\n",
      "Epoch:74 / batch:100 / loss 0.02\n",
      "Epoch 74 / time 0:00:37.533359 / loss 0.05 / val_loss 0.97 / acc 90.13 / val_acc 80.96\n",
      "Epoch    73: reducing learning rate of group 0 to 1.5953e-04.\n",
      "Epoch:75 / batch:50 / loss 0.02\n",
      "Epoch:75 / batch:100 / loss 0.02\n",
      "Epoch 75 / time 0:00:37.520675 / loss 0.05 / val_loss 0.97 / acc 90.25 / val_acc 80.79\n",
      "Epoch:76 / batch:50 / loss 0.02\n",
      "Epoch:76 / batch:100 / loss 0.02\n",
      "Epoch 76 / time 0:00:37.538794 / loss 0.05 / val_loss 0.97 / acc 90.38 / val_acc 80.79\n",
      "Epoch    75: reducing learning rate of group 0 to 1.4358e-04.\n",
      "Epoch:77 / batch:50 / loss 0.02\n",
      "Epoch:77 / batch:100 / loss 0.02\n",
      "Epoch 77 / time 0:00:37.497630 / loss 0.05 / val_loss 0.97 / acc 90.50 / val_acc 80.79\n",
      "Epoch:78 / batch:50 / loss 0.02\n",
      "Epoch:78 / batch:100 / loss 0.02\n",
      "Epoch 78 / time 0:00:37.480831 / loss 0.05 / val_loss 0.97 / acc 90.61 / val_acc 80.84\n",
      "Epoch    77: reducing learning rate of group 0 to 1.2922e-04.\n",
      "Epoch:79 / batch:50 / loss 0.02\n",
      "Epoch:79 / batch:100 / loss 0.02\n",
      "Epoch 79 / time 0:00:37.528232 / loss 0.05 / val_loss 0.98 / acc 90.73 / val_acc 80.79\n",
      "Epoch:80 / batch:50 / loss 0.02\n",
      "Epoch:80 / batch:100 / loss 0.02\n",
      "Epoch 80 / time 0:00:37.535573 / loss 0.05 / val_loss 0.98 / acc 90.84 / val_acc 80.73\n",
      "Epoch    79: reducing learning rate of group 0 to 1.1630e-04.\n",
      "Epoch:81 / batch:50 / loss 0.02\n",
      "Epoch:81 / batch:100 / loss 0.02\n",
      "Epoch 81 / time 0:00:37.524405 / loss 0.05 / val_loss 0.98 / acc 90.95 / val_acc 80.67\n",
      "Epoch:82 / batch:50 / loss 0.02\n",
      "Epoch:82 / batch:100 / loss 0.01\n",
      "Epoch 82 / time 0:00:37.505420 / loss 0.04 / val_loss 0.98 / acc 91.06 / val_acc 80.61\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0467e-04.\n",
      "Epoch:83 / batch:50 / loss 0.02\n",
      "Epoch:83 / batch:100 / loss 0.01\n",
      "Epoch 83 / time 0:00:37.444665 / loss 0.04 / val_loss 0.98 / acc 91.16 / val_acc 80.61\n",
      "Epoch:84 / batch:50 / loss 0.02\n",
      "Epoch:84 / batch:100 / loss 0.01\n",
      "Epoch 84 / time 0:00:37.501592 / loss 0.04 / val_loss 0.98 / acc 91.26 / val_acc 80.61\n",
      "Epoch    83: reducing learning rate of group 0 to 9.4203e-05.\n",
      "Epoch:85 / batch:50 / loss 0.02\n",
      "Epoch:85 / batch:100 / loss 0.01\n",
      "Epoch 85 / time 0:00:37.482404 / loss 0.04 / val_loss 0.98 / acc 91.36 / val_acc 80.49\n",
      "Epoch:86 / batch:50 / loss 0.02\n",
      "Epoch:86 / batch:100 / loss 0.01\n",
      "Epoch 86 / time 0:00:37.471162 / loss 0.04 / val_loss 0.98 / acc 91.46 / val_acc 80.55\n",
      "Epoch    85: reducing learning rate of group 0 to 8.4782e-05.\n",
      "Epoch:87 / batch:50 / loss 0.02\n",
      "Epoch:87 / batch:100 / loss 0.01\n",
      "Epoch 87 / time 0:00:37.384479 / loss 0.04 / val_loss 0.98 / acc 91.55 / val_acc 80.49\n",
      "Epoch:88 / batch:50 / loss 0.02\n",
      "Epoch:88 / batch:100 / loss 0.01\n",
      "Epoch 88 / time 0:00:37.353557 / loss 0.04 / val_loss 0.98 / acc 91.65 / val_acc 80.49\n",
      "Epoch    87: reducing learning rate of group 0 to 7.6304e-05.\n",
      "Epoch:89 / batch:50 / loss 0.02\n",
      "Epoch:89 / batch:100 / loss 0.01\n",
      "Epoch 89 / time 0:00:37.429930 / loss 0.04 / val_loss 0.98 / acc 91.74 / val_acc 80.49\n",
      "Epoch:90 / batch:50 / loss 0.02\n",
      "Epoch:90 / batch:100 / loss 0.01\n",
      "Epoch 90 / time 0:00:37.433743 / loss 0.04 / val_loss 0.98 / acc 91.83 / val_acc 80.49\n",
      "Epoch    89: reducing learning rate of group 0 to 6.8674e-05.\n",
      "Epoch:91 / batch:50 / loss 0.02\n",
      "Epoch:91 / batch:100 / loss 0.01\n",
      "Epoch 91 / time 0:00:37.490796 / loss 0.04 / val_loss 0.98 / acc 91.91 / val_acc 80.49\n",
      "Epoch:92 / batch:50 / loss 0.02\n",
      "Epoch:92 / batch:100 / loss 0.01\n",
      "Epoch 92 / time 0:00:37.470664 / loss 0.04 / val_loss 0.99 / acc 92.00 / val_acc 80.49\n",
      "Epoch    91: reducing learning rate of group 0 to 6.1806e-05.\n",
      "Epoch:93 / batch:50 / loss 0.02\n",
      "Epoch:93 / batch:100 / loss 0.01\n",
      "Epoch 93 / time 0:00:37.379364 / loss 0.04 / val_loss 0.99 / acc 92.08 / val_acc 80.43\n",
      "Epoch:94 / batch:50 / loss 0.02\n",
      "Epoch:94 / batch:100 / loss 0.01\n",
      "Epoch 94 / time 0:00:37.513131 / loss 0.04 / val_loss 0.99 / acc 92.17 / val_acc 80.43\n",
      "Epoch    93: reducing learning rate of group 0 to 5.5626e-05.\n",
      "Epoch:95 / batch:50 / loss 0.02\n",
      "Epoch:95 / batch:100 / loss 0.01\n",
      "Epoch 95 / time 0:00:37.535481 / loss 0.04 / val_loss 0.99 / acc 92.25 / val_acc 80.49\n",
      "Epoch:96 / batch:50 / loss 0.02\n",
      "Epoch:96 / batch:100 / loss 0.01\n",
      "Epoch 96 / time 0:00:37.430469 / loss 0.04 / val_loss 0.99 / acc 92.32 / val_acc 80.49\n",
      "Epoch    95: reducing learning rate of group 0 to 5.0063e-05.\n",
      "Epoch:97 / batch:50 / loss 0.02\n",
      "Epoch:97 / batch:100 / loss 0.01\n",
      "Epoch 97 / time 0:00:37.377189 / loss 0.04 / val_loss 0.99 / acc 92.40 / val_acc 80.49\n",
      "Epoch:98 / batch:50 / loss 0.02\n",
      "Epoch:98 / batch:100 / loss 0.01\n",
      "Epoch 98 / time 0:00:37.468563 / loss 0.04 / val_loss 0.99 / acc 92.48 / val_acc 80.49\n",
      "Epoch    97: reducing learning rate of group 0 to 4.5057e-05.\n",
      "Epoch:99 / batch:50 / loss 0.02\n",
      "Epoch:99 / batch:100 / loss 0.01\n",
      "Epoch 99 / time 0:00:37.461420 / loss 0.04 / val_loss 0.99 / acc 92.55 / val_acc 80.49\n",
      "Epoch:100 / batch:50 / loss 0.02\n",
      "Epoch:100 / batch:100 / loss 0.01\n",
      "Epoch 100 / time 0:00:37.525875 / loss 0.04 / val_loss 0.99 / acc 92.62 / val_acc 80.37\n",
      "Epoch    99: reducing learning rate of group 0 to 4.0551e-05.\n"
     ]
    }
   ],
   "source": [
    "writer_t = SummaryWriter(comment=' Train / epoch : {} / learning_rate : {}'.format(epochs,lr))\n",
    "writer_v = SummaryWriter(comment=' Val / epoch : {} / learning_rate : {}'.format(epochs,lr))\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "for i in range(epochs):\n",
    "    start_time = datetime.now()\n",
    "    for j, (data, label) in enumerate(Train_loader):\n",
    "        x = data.to(device)\n",
    "        y_ = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)\n",
    "        loss = loss_func(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(output.max().shape,y_.shape)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct_train += (predicted == y_).sum().item()\n",
    "        total_train += y_.size(0)\n",
    "        accuracy_train = (100 * correct_train / total_train)\n",
    "#         print('a')\n",
    "        \n",
    "        if (j+1) % 50 == 0:\n",
    "            print('Epoch:{} / batch:{} / loss {:.2f}'.format(i+1,j+1, loss))\n",
    "    with torch.no_grad():\n",
    "        for val_x, val_y in Valid_loader:\n",
    "            val_x = val_x.to(device)\n",
    "            val_y = val_y.to(device)\n",
    "            val_output = model.forward(val_x)\n",
    "            v_loss = loss_func(val_output, val_y)\n",
    "            _, predicted_val = torch.max(val_output.data, 1)\n",
    "            accuracy_val = ( 100 * (predicted_val == val_y).sum().item() / len(val_x))\n",
    "    end_time = datetime.now()    \n",
    "    print('Epoch {} / time {} / loss {:.2f} / val_loss {:.2f} / acc {:.2f} / val_acc {:.2f}'.format(i+1, end_time - start_time, loss, v_loss, accuracy_train, accuracy_val))\n",
    "    scheduler.step(v_loss, i)\n",
    "#     if i == 0:\n",
    "#       grid = torchvision.utils.make_grid(data)\n",
    "#       writer.add_image('images',grid, i)\n",
    "    writer_t.add_scalar('Loss', loss, i+1)\n",
    "    writer_v.add_scalar('Loss', v_loss, i+1)\n",
    "    writer_t.add_scalar('Acc', accuracy_train, i+1)\n",
    "    writer_v.add_scalar('Acc', accuracy_val, i+1)\n",
    "\n",
    "writer_t.close()\n",
    "writer_v.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        x = image.to(device)\n",
    "        y_ = label.to(device)\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        _, output_index = torch.max(output,1)\n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += (output_index == y_).sum().float()\n",
    "        \n",
    "    print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
